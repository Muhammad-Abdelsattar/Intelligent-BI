Directory structure:
└── project/
    ├── README.md
    ├── pyproject.toml
    ├── setup.py
    ├── core/
    │   ├── __init__.py
    │   ├── config.py
    │   ├── main.py
    │   ├── agents/
    │   │   ├── __init__.py
    │   │   └── sql_generator.py
    │   ├── config/
    │   │   ├── db_config.yaml
    │   │   └── llm_providers.yaml
    │   ├── database/
    │   │   ├── __init__.py
    │   │   ├── executor.py
    │   │   ├── manager.py
    │   │   ├── service.py
    │   │   ├── strategy.py
    │   │   └── validator.py
    │   ├── llm/
    │   │   ├── __init__.py
    │   │   ├── llm_factory.py
    │   │   ├── llm_service.py
    │   │   └── prompt_manager.py
    │   ├── models/
    │   │   └── __init__.py
    │   ├── prompts/
    │   │   ├── analysis_reporter/
    │   │   │   ├── system.prompt
    │   │   │   └── user.prompt
    │   │   ├── chart_generator/
    │   │   │   ├── system.prompt
    │   │   │   └── user.prompt
    │   │   └── sql_generator/
    │   │       ├── system.prompt
    │   │       └── user.prompt
    │   ├── utils/
    │   │   └── __init__.py
    │   └── workflows/
    │       └── __init__.py
    └── tests/
        ├── __init__.py
        ├── test_agents.py
        └── test_database.py

================================================
FILE: README.md
================================================
# BI Agent Project

This project contains the core logic for a Business Intelligence AI Agent.

## Development Setup

To install the project in editable mode, run the following command from the project root:

```bash
pip install -e .
```



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "bi-agent-core"
version = "0.1.0"
description = "Core functionality for the BI Agent project."
readme = "README.md"
requires-python = ">=3.8"
license = {text = "MIT"}
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "langchain",
    "langchain-core",
    "omegaconf",
    "SQLAlchemy",
    "pandas",
    "sqlparse",
]

[tool.setuptools.packages.find]
where = ["."]
include = ["core*", "interface*"]



================================================
FILE: setup.py
================================================
from setuptools import setup

setup()



================================================
FILE: core/__init__.py
================================================
[Empty file]


================================================
FILE: core/config.py
================================================
[Empty file]


================================================
FILE: core/main.py
================================================
import os
from omegaconf import OmegaConf

# Register the 'env' resolver for OmegaConf to resolve environment variables
OmegaConf.register_resolver("env", lambda name: os.environ.get(name))

# Example of loading the database configuration
if __name__ == "__main__":
    # Set some dummy environment variables for testing
    os.environ["DB_HOST"] = "localhost"
    os.environ["DB_PORT"] = "5432"
    os.environ["DB_USER"] = "test_user"
    os.environ["DB_PASSWORD"] = "test_password"
    os.environ["DB_NAME"] = "test_db"

    try:
        # Load the configuration from db_config.yaml
        config = OmegaConf.load("core/config/db_config.yaml")

        # Access the database configuration
        db_config = config.database

        print("Successfully loaded database configuration:")
        print(OmegaConf.to_yaml(db_config))

        # You can now use db_config to initialize your DatabaseService
        # from core.database import DatabaseService
        # db_service = DatabaseService(db_config)
        # print(f"Database URI: {db_service._manager.get_uri()}")

    except Exception as e:
        print(f"Error loading configuration: {e}")
        print("Please ensure environment variables are set and db_config.yaml exists.")



================================================
FILE: core/agents/__init__.py
================================================
from .sql_generator import SQLGeneratorAgent

__all__ = ["SQLGeneratorAgent"]



================================================
FILE: core/agents/sql_generator.py
================================================
from omegaconf import DictConfig
import pandas as pd

from core.llm import LLMService
from core.database import DatabaseService

class SQLGeneratorAgent:
    """
    An agent responsible for generating and executing SQL queries based on natural language questions.
    """
    def __init__(self, agent_config: DictConfig, db_config: DictConfig):
        """
        Initializes the SQLGeneratorAgent.

        Args:
            agent_config: Configuration specific to this agent (e.g., LLM provider key).
            db_config: Configuration for the database connection.
        """
        self.llm_service = LLMService(
            agent_name=agent_config.name, # Now configurable
            provider_key=agent_config.llm_provider_key,
            llm_config_path=agent_config.llm_config_path,
            prompts_base_path=agent_config.prompts_base_path
        )

        self.db_service = DatabaseService(db_config)

        self.agent_config = agent_config

    def generate_and_execute_sql(self, natural_language_question: str, database_dialect: str) -> pd.DataFrame:
        """
        Generates an SQL query from a natural language question, validates it, and executes it.

        Args:
            natural_language_question: The question in natural language.
            database_dialect: The SQL dialect of the target database (e.g., "PostgreSQL", "SQLite").

        Returns:
            A pandas DataFrame containing the results of the SQL query.

        Raises:
            ValueError: If the LLM fails to generate SQL or validation fails.
            RuntimeError: If SQL execution fails.
        """
        # 1. Get database schema
        schema_definition = self.db_service.get_schema_info()

        # 2. Prepare variables for LLM prompt
        llm_variables = {
            "schema_definition": schema_definition,
            "user_question": natural_language_question,
            "database_dialect": database_dialect
        }

        # 3. Generate SQL query using LLMService
        generated_sql = self.llm_service.generate_text(llm_variables)

        # Check if LLM returned an error message (as per prompt instructions)
        if generated_sql.strip().startswith("Error:"):
            raise ValueError(f"LLM failed to generate SQL: {generated_sql}")

        # 4. Validate and execute SQL query using DatabaseService
        try:
            results_df = self.db_service.execute_validated_query(generated_sql)
            return results_df
        except ValueError as e:
            raise ValueError(f"Generated SQL validation failed: {e}. Query: {generated_sql}")
        except RuntimeError as e:
            raise RuntimeError(f"SQL execution failed: {e}. Query: {generated_sql}")



================================================
FILE: core/config/db_config.yaml
================================================
database:
  type: postgres
  params:
    host: ${oc.env:DB_HOST}
    port: ${oc.env:DB_PORT}
    user: ${oc.env:DB_USER}
    password: ${oc.env:DB_PASSWORD}
    dbname: ${oc.env:DB_NAME}



================================================
FILE: core/config/llm_providers.yaml
================================================
llm_providers:
  azure-openai-gpt4o-mini:
    display_name: "Azure OpenAI (GPT-4o mini)"
    class: "langchain_openai.AzureChatOpenAI"
    params:
      temperature: 0.3
      openai_api_version: ${oc.env:OPENAI_API_VERSION}
      azure_deployment: ${oc.env:AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}
      azure_endpoint: ${oc.env:AZURE_OPENAI_ENDPOINT}
      api_key: ${oc.env:AZURE_OPENAI_API_KEY}

  google-gemini-2.5-lite:
    display_name: "Google Gemini (2.5 Flash Lite)"
    class: "langchain_google_genai.ChatGoogleGenerativeAI"
    params:
      temperature: 0.3
      model: "gemini-2.5-flash-lite"
      google_api_key: ${oc.env:GOOGLE_API_KEY}

  google-gemini-2.5-flash:
    display_name: "Google Gemini (2.5 Flash)"
    class: "langchain_google_genai.ChatGoogleGenerativeAI"
    params:
      temperature: 0.3
      model: "gemini-2.5-flash"
      google_api_key: ${oc.env:GOOGLE_API_KEY}
      thinking_budget: 0


================================================
FILE: core/database/__init__.py
================================================
from .manager import DatabaseManager
from .executor import DatabaseExecutor
from .service import DatabaseService

__all__ = ["DatabaseManager", "DatabaseExecutor", "DatabaseService"]



================================================
FILE: core/database/executor.py
================================================
import pandas as pd
from sqlalchemy.engine import Engine
from sqlalchemy.exc import SQLAlchemyError
from .manager import DatabaseManager

class DatabaseExecutor:
    """
    Handles the execution of SQL queries against the database.
    """
    def __init__(self, engine: Engine):
        self._engine = engine

    def execute_query(self, sql_query: str) -> pd.DataFrame:
        """
        Executes a read-only SQL query and returns the results as a pandas DataFrame.

        Args:
            sql_query: The SQL SELECT statement to execute.

        Returns:
            A pandas DataFrame containing the query results.

        Raises:
            RuntimeError: If there is a database execution error.
        """
        try:
            with self._engine.connect() as connection:
                df = pd.read_sql_query(sql_query, connection.connection)
            return df
        except SQLAlchemyError as e:
            raise RuntimeError(f"Database execution failed: {e}") from e



================================================
FILE: core/database/manager.py
================================================
from __future__ import annotations
from omegaconf import DictConfig
from sqlalchemy import create_engine
from sqlalchemy.engine import Engine

from .strategy import DatabaseConnectionStrategy, PostgresConnectionStrategy, SqliteConnectionStrategy

class DatabaseManager:
    """
    Manages database connections via a strategy and acts as a factory for itself.
    """
    def __init__(self, strategy: DatabaseConnectionStrategy):
        if not isinstance(strategy, DatabaseConnectionStrategy):
            raise TypeError("A valid DatabaseConnectionStrategy object must be provided.")
        self._strategy = strategy
        self._engine = create_engine(self._strategy.get_uri())

    @classmethod
    def from_config(cls, db_config: DictConfig) -> DatabaseManager:
        """
        Factory class method to build a DatabaseManager with the correct strategy
        based on the provided configuration.
        """
        db_type = db_config.get("type")
        if not db_type:
            raise ValueError("Database 'type' must be specified in the configuration.")

        strategy: DatabaseConnectionStrategy
        params = db_config.get("params", {})

        if db_type == "postgres":
            strategy = PostgresConnectionStrategy(**params)
        elif db_type == "sqlite":
            strategy = SqliteConnectionStrategy(**params)
        else:
            raise NotImplementedError(f"Database type '{db_type}' is not supported.")

        return cls(strategy)

    def get_engine(self) -> Engine:
        """Returns the SQLAlchemy engine instance."""
        return self._engine

    def get_uri(self) -> str:
        """Returns the database connection URI."""
        return self._strategy.get_uri()

    def get_schema_info(self) -> str:
        """Returns the database schema information from the strategy."""
        return self._strategy.get_schema_info()



================================================
FILE: core/database/service.py
================================================
import pandas as pd
from omegaconf import DictConfig
import sqlparse

from .manager import DatabaseManager
from .validator import SQLValidator
from .executor import DatabaseExecutor

class DatabaseService:
    """
    A high-level service that orchestrates database operations,
    combining connection management, query validation, and execution.
    """
    def __init__(self, db_config: DictConfig):
        """
        Initializes the DatabaseService by setting up the manager, validator, and executor.

        Args:
            db_config: OmegaConf DictConfig containing database connection details.
        """
        self._manager = DatabaseManager.from_config(db_config)
        self._validator = SQLValidator(self._manager.get_uri())
        self._executor = DatabaseExecutor(self._manager.get_engine())

    def execute_validated_query(self, sql_query: str) -> pd.DataFrame:
        """
        Validates an SQL query and, if valid, executes it.

        Args:
            sql_query: The SQL query string to validate and execute.

        Returns:
            A pandas DataFrame containing the query results.

        Raises:
            ValueError: If the query is invalid or not a SELECT statement.
            RuntimeError: If there is a database execution error.
        """
        is_valid, message = self._validator.verify_executable_select_query(sql_query)
        if not is_valid:
            raise ValueError(f"Invalid SQL query: {message}")

        return self._executor.execute_query(sql_query)

    def get_schema_info(self) -> str:
        """
        Retrieves the database schema information from the underlying strategy.

        Returns:
            A string representation of the database schema.
        """
        return self._manager.get_schema_info()



================================================
FILE: core/database/strategy.py
================================================
from sqlalchemy import create_engine, inspect

# --- Abstract Strategy ---
class DatabaseConnectionStrategy(ABC):
    """Abstract base class for a database connection strategy."""
    @abstractmethod
    def get_uri(self) -> str:
        """Constructs the SQLAlchemy database URI."""
        pass

    @abstractmethod
    def get_schema_info(self) -> str:
        """Fetches and returns the database schema information as a string."""
        pass

# --- Concrete Strategies ---
@dataclass
class PostgresConnectionStrategy(DatabaseConnectionStrategy):
    """Strategy for connecting to a PostgreSQL database."""
    host: str
    port: int
    user: str
    password: str
    dbname: str

    def get_uri(self) -> str:
        # Assumes psycopg2 driver
        return f"postgresql+psycopg2://{self.user}:{self.password}@{self.host}:{self.port}/{self.dbname}"

    def get_schema_info(self) -> str:
        engine = create_engine(self.get_uri())
        inspector = inspect(engine)
        schema_info = []
        for table_name in inspector.get_table_names():
            schema_info.append(f"TABLE {table_name}:")
            for column in inspector.get_columns(table_name):
                schema_info.append(f"  {column['name']} {column['type']}")
        return "\n".join(schema_info)

@dataclass
class SqliteConnectionStrategy(DatabaseConnectionStrategy):
    """Strategy for connecting to a SQLite database."""
    db_path: str

    def get_uri(self) -> str:
        return f"sqlite:///{self.db_path}"

    def get_schema_info(self) -> str:
        engine = create_engine(self.get_uri())
        inspector = inspect(engine)
        schema_info = []
        for table_name in inspector.get_table_names():
            schema_info.append(f"TABLE {table_name}:")
            for column in inspector.get_columns(table_name):
                schema_info.append(f"  {column['name']} {column['type']}")
        return "\n".join(schema_info)



================================================
FILE: core/database/validator.py
================================================
from sqlalchemy import create_engine, text
from sqlalchemy.exc import ProgrammingError
import sqlparse

class SQLValidator:
    """
    Validates SQL queries by attempting to generate an execution plan using EXPLAIN.
    """
    def __init__(self, db_uri: str):
        """
        Initializes the validator with a database connection URI.

        Args:
            db_uri: A SQLAlchemy-compatible database URI
                    (e.g., 'sqlite:///mydatabase.db', 'postgresql://user:pass@host/dbname').
        """
        try:
            self.engine = create_engine(db_uri)
        except ImportError:
            raise ImportError("SQLAlchemy is not installed. Please install it with 'pip install SQLAlchemy'.")

    def verify_executable_select_query(self, sql_query: str) -> tuple[bool, str]:
        """
        Validates the SQL query syntax by asking the database to create an
        execution plan. It only validates SELECT statements.

        Args:
            sql_query: The SQL query string to validate.

        Returns:
            A tuple (is_valid: bool, message: str).
            The message will be 'OK' on success or an error message on failure.
        """
        try:
            statement_type = sqlparse.parse(sql_query)[0].get_type()
            if statement_type != 'SELECT':
                return False, f"Error: Query must be a SELECT statement, but it is a {statement_type} statement."
        except IndexError:
            return False, "Error: The SQL query is empty or invalid."

        query = sql_query.strip()

        try:
            with self.engine.connect() as connection:
                # Using text() is important for safely passing the query.
                # The EXPLAIN statement asks the DB to parse and plan the query.
                connection.execute(text(f"EXPLAIN {query}"))
            return True, "OK"
        except ProgrammingError as e:
            # This exception is commonly raised for syntax errors.
            return False, f"Syntax Error: {e.orig}"
        except Exception as e:
            return False, f"An unexpected error occurred during validation: {e}"



================================================
FILE: core/llm/__init__.py
================================================
from .llm_factory import LLMFactory
from .prompt_manager import PromptManager
from .llm_service import LLMService

__all__ = [
    "LLMFactory",
    "PromptManager",
    "LLMService",
]



================================================
FILE: core/llm/llm_factory.py
================================================
import importlib
from typing import Dict, Any

from omegaconf import OmegaConf, DictConfig
from langchain_core.language_models.chat_models import BaseChatModel

class LLMFactory:
    """
    A factory class for creating LangChain LLM clients using OmegaConf.
    """

    def __init__(self, config_path: str = "core/config/llm_providers.yaml"):
        """
        Initializes the factory by loading the LLM provider configuration.

        Args:
            config_path: The path to the OmegaConf YAML file.
        """
        self._config = self._load_config(config_path)

    def _load_config(self, config_path: str) -> DictConfig:
        """Loads the model configuration from the YAML file using OmegaConf."""
        try:
            conf = OmegaConf.load(config_path)
            if not isinstance(conf, DictConfig) or 'llm_providers' not in conf or not isinstance(conf.llm_providers, DictConfig):
                raise ValueError("YAML config must be a dictionary and contain a 'llm_providers' dictionary.")
            return conf.llm_providers
        except FileNotFoundError:
            raise FileNotFoundError(f"LLM configuration file not found at: {config_path}")
        except Exception as e:
            raise RuntimeError(f"Error parsing LLM configuration with OmegaConf from '{config_path}': {e}")

    def get_available_providers(self) -> Dict[str, str]:
        """
        Returns a dictionary of available provider keys and their display names.
        Reads from the pre-loaded OmegaConf configuration object.
        """
        if not self._config:
            return {}
        return {
            key: provider.get('display_name', key)
            for key, provider in self._config.items()
            if provider
        }

    def create_llm_client(self, provider_key: str) -> BaseChatModel:
        """
        Creates an LLM client instance based on the provider key.

        Args:
            provider_key: The key from the llm_providers.yaml file (e.g., 'azure_openai_4o').

        Returns:
            An instance of the specified LangChain LLM client.
        """
        if not self._config or provider_key not in self._config:
            raise ValueError(f"Provider '{provider_key}' not found in the configuration. "
                             f"Available providers: {list(self._config.keys() if self._config else [])}")

        provider_config = self._config.get(provider_key)

        if 'class' not in provider_config or 'params' not in provider_config:
            raise ValueError(f"Provider '{provider_key}' configuration is missing 'class' or 'params'.")

        resolved_params = OmegaConf.to_container(provider_config.params, resolve=True)

        module_path, class_name = provider_config['class'].rsplit('.', 1)
        try:
            module = importlib.import_module(module_path)
            llm_class = getattr(module, class_name)
        except ImportError:
            raise ImportError(f"Could not import module '{module_path}' for LLM provider '{provider_key}'.")
        except AttributeError:
            raise AttributeError(f"Could not find class '{class_name}' in module '{module_path}'.")
        except Exception as e:
            raise RuntimeError(f"An unexpected error occurred while loading the LLM class: {e}")

        try:
            return llm_class(**resolved_params)
        except TypeError as e:
            raise TypeError(f"Failed to instantiate LLM client for '{provider_key}'. "
                            f"Check if the parameters in the config match the class constructor. Error: {e}")


================================================
FILE: core/llm/llm_service.py
================================================
from typing import Dict, Any, List, Optional, Type, Union
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from pydantic import BaseModel

from core.llm.llm_factory import LLMFactory
from core.llm.prompt_manager import PromptManager

class LLMService:
    """
    A high-level interface for AI agents to interact with the LLM.

    This class abstracts away the details of prompt management and LLM client
    creation, providing methods for different types of generation (text, structured).
    """

    def __init__(self, agent_name: str, provider_key: str, llm_config_path: str = "core/config/llm_providers.yaml", prompts_base_path: str = "core/prompts"):
        """
        Initializes the complete LLM stack for a specific agent.

        Args:
            agent_name: The name of the agent (e.g., 'sql_generator').
            provider_key: The key for the LLM provider from the config
                          (e.g., 'azure_openai_4o').
            llm_config_path: Path to the LLM providers configuration file.
            prompts_base_path: Path to the prompts directory.
        """
        # 1. Create LLM client
        llm_factory = LLMFactory(config_path=llm_config_path)
        self.llm_client = llm_factory.create_llm_client(provider_key)

        # 2. Load prompts and examples
        prompt_manager = PromptManager(prompts_base_path=prompts_base_path)
        self.system_prompt_template, self.human_prompt_template = prompt_manager.get_prompts(agent_name)
        self.few_shot_examples = prompt_manager.get_few_shot_examples(agent_name)

    def _build_messages(self, variables: Dict[str, Any]) -> List[BaseMessage]:
        """Helper to build the list of messages for the LLM."""
        messages: List[BaseMessage] = []

        # 1. System Message
        system_content = self.system_prompt_template.format(**variables)
        messages.append(SystemMessage(content=system_content))

        # 2. Few-shot examples
        if self.few_shot_examples:
            for example in self.few_shot_examples:
                if 'user' in example and 'assistant' in example:
                    messages.append(HumanMessage(content=example['user']))
                    messages.append(AIMessage(content=example['assistant']))

        # 3. Human Message
        human_content = self.human_prompt_template.format(**variables)
        messages.append(HumanMessage(content=human_content))
        return messages

    def _extract_sql_from_response(self, raw_response: AIMessage) -> str:
        """
        Extracts the SQL query from a markdown code block in the raw LLM response.
        Assumes the SQL is within ```sql ... ```.
        """
        start_tag = "```sql"
        end_tag = "```"
        raw_response = raw_response.content

        start_index = raw_response.find(start_tag)
        if start_index == -1:
            # If no start tag, check for error message
            if raw_response.strip().startswith("Error:"):
                return raw_response # Return error message as is
            raise ValueError("LLM response does not contain a '```sql' block.")

        # Adjust start_index to point to the character after the start_tag
        start_index += len(start_tag)

        end_index = raw_response.find(end_tag, start_index)
        if end_index == -1:
            raise ValueError("LLM response contains '```sql' but no closing '```' tag.")

        extracted_sql = raw_response[start_index:end_index].strip()
        return extracted_sql

    def generate_text(self, variables: Dict[str, Any]) -> str:
        """
        Generates a text response from the LLM.
        """
        messages = self._build_messages(variables)
        raw_response = self.llm_client.invoke(messages)
        return self._extract_sql_from_response(raw_response)

    def generate_structured(self, variables: Dict[str, Any], response_model: Type[BaseModel]) -> BaseModel:
        """
        Generates a structured response from the LLM, parsed into a Pydantic model.
        """
        messages = self._build_messages(variables)
        structured_llm = self.llm_client.with_structured_output(response_model)
        structured_response = structured_llm.invoke(messages)
        return structured_response



================================================
FILE: core/llm/prompt_manager.py
================================================
import json
from pathlib import Path
from typing import Tuple, List, Dict, Optional, Union

class PromptManager:
    """
    Manages loading and providing prompt templates for different agents
    """

    def __init__(self, prompts_base_path: Union[str, Path] = "core/prompts"):
        """
        Initializes the PromptManager with the base path for prompts.

        Args:
            prompts_base_path: The root directory where agent-specific prompt
                               folders are located.
        """
        self.prompts_base_path = Path(prompts_base_path)
        if not self.prompts_base_path.is_dir():
            raise FileNotFoundError(f"Prompts base directory not found at: {self.prompts_base_path}")

    def _read_file(self, path: Path) -> str:
        """Reads a file and returns its content."""
        try:
            return path.read_text(encoding='utf-8')
        except FileNotFoundError:
            raise FileNotFoundError(f"Prompt file not found at: {path}")
        except Exception as e:
            raise IOError(f"Error reading prompt file at {path}: {e}")

    def get_prompts(self, agent_name: str) -> Tuple[str, str]:
        """
        Loads the system and user prompt templates for a given agent.

        Args:
            agent_name: The name of the agent, corresponding to a folder
                        in the prompts_base_path.

        Returns:
            A tuple containing the system prompt and user prompt templates.
        """
        agent_prompt_dir = self.prompts_base_path / agent_name
        if not agent_prompt_dir.is_dir():
            raise FileNotFoundError(f"Prompt directory for agent '{agent_name}' not found at {agent_prompt_dir}")

        system_prompt_path = agent_prompt_dir / "system.prompt"
        user_prompt_path = agent_prompt_dir / "user.prompt"

        system_prompt = self._read_file(system_prompt_path)
        user_prompt = self._read_file(user_prompt_path)

        return system_prompt, user_prompt

    def get_few_shot_examples(self, agent_name: str) -> Optional[List[Dict[str, str]]]:
        """
        Loads optional few-shot examples for a given agent from a JSON file.

        The JSON file should be a list of dictionaries, where each dictionary
        represents an example (e.g., {"user": "...", "assistant": "..."}).

        Args:
            agent_name: The name of the agent.

        Returns:
            A list of few-shot examples, or None if the file doesn't exist.
        """
        examples_path = self.prompts_base_path / agent_name / "few_shot_examples.json"
        if not examples_path.exists():
            return None

        try:
            with examples_path.open('r', encoding='utf-8') as f:
                examples = json.load(f)
            if not isinstance(examples, list):
                raise ValueError("Few-shot examples file must contain a JSON list.")
            return examples
        except json.JSONDecodeError as e:
            raise ValueError(f"Error decoding JSON from {examples_path}: {e}")
        except Exception as e:
            raise IOError(f"Error reading few-shot examples file at {examples_path}: {e}")



================================================
FILE: core/models/__init__.py
================================================
[Empty file]


================================================
FILE: core/prompts/analysis_reporter/system.prompt
================================================
You are a data analysis expert. Your task is to interpret the results of a SQL query and provide a clear, concise, and insightful analysis for a business user.


================================================
FILE: core/prompts/analysis_reporter/user.prompt
================================================
Please analyze the following data, which was retrieved using the query:

**SQL Query:**
{sql_query}

**Data:**
{query_results}

**Analysis:**


================================================
FILE: core/prompts/chart_generator/system.prompt
================================================
You are a data visualization specialist. Your job is to suggest the most appropriate chart to represent a given dataset and provide the necessary configuration in a specified JSON format.


================================================
FILE: core/prompts/chart_generator/user.prompt
================================================
Given the following data and analysis, please suggest a chart type and provide its configuration.

**Analysis:**
{analysis}

**Data:**
{query_results}

**Chart Configuration (JSON):**


================================================
FILE: core/prompts/sql_generator/system.prompt
================================================
You are a world-class SQL generation expert. Your sole purpose is to convert natural language questions into accurate, efficient, and executable SQL queries.

You must adhere to the following rules strictly:
1.  **SQL Dialect:** The generated query must be for the {database_dialect} dialect.
2.  **Read-Only Operations:** You must only generate `SELECT` statements. Under no circumstances should you generate any data-modifying statements (`INSERT`, `UPDATE`, `DELETE`, `DROP`, `TRUNCATE`, etc.).
3.  **Schema Adherence:** The query must only use the tables and columns provided in the database schema. Do not invent tables, columns, or functions that are not in the schema.
4.  **Correctness and Efficiency:** The query must be syntactically correct and logically sound. It should answer the user's question accurately. Write efficient queries using CTEs for complex logic and appropriate JOINs.
5.  **Output Format:** You MUST wrap the generated SQL query in a markdown code block like this:
    ```sql
    SELECT ...
    ```
    Your output should contain nothing else outside of this block.
6.  **Handling Ambiguity:** If the user's question is ambiguous or cannot be answered with the provided schema, you must not generate a query. Instead, output a single line starting with "Error:" followed by a concise explanation. For example: "Error: The provided schema does not contain information about customer churn."

Review the provided database schema and the user's question carefully before generating the SQL query.


================================================
FILE: core/prompts/sql_generator/user.prompt
================================================
**Database Dialect:**
{database_dialect}

**Database Schema:**
```
{schema_definition}
```

**User Question:**
{user_question}

**Generated SQL Query:**


================================================
FILE: core/utils/__init__.py
================================================
[Empty file]


================================================
FILE: core/workflows/__init__.py
================================================
[Empty file]


================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/test_agents.py
================================================
[Empty file]


================================================
FILE: tests/test_database.py
================================================
[Empty file]

