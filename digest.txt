Directory structure:
└── core/
    ├── __init__.py
    ├── config.py
    ├── main.py
    ├── agents/
    │   ├── __init__.py
    │   └── sql_agent.py
    ├── config/
    │   ├── agents.yaml
    │   ├── databases.yaml
    │   └── llms.yaml
    ├── database/
    │   ├── __init__.py
    │   ├── manager.py
    │   ├── service.py
    │   └── strategy.py
    ├── llm/
    │   ├── __init__.py
    │   ├── llm_factory.py
    │   ├── llm_service.py
    │   └── prompt_manager.py
    ├── models/
    │   └── __init__.py
    ├── prompts/
    │   ├── analysis_reporter/
    │   │   ├── system.prompt
    │   │   └── user.prompt
    │   ├── chart_generator/
    │   │   ├── system.prompt
    │   │   └── user.prompt
    │   └── sql_generator/
    │       ├── system.prompt
    │       └── user.prompt
    ├── utils/
    │   ├── __init__.py
    │   └── config_parser.py
    └── workflows/
        └── __init__.py

================================================
FILE: __init__.py
================================================
[Empty file]


================================================
FILE: config.py
================================================
[Empty file]


================================================
FILE: main.py
================================================
import os
import logging
import sys
from omegaconf import OmegaConf, DictConfig
from pathlib import Path

# --- Logging Setup ---
# Configure logging to output to stdout for better visibility in containerized environments
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

# --- Configuration Loading ---
OmegaConf.register_resolver("env", lambda name: os.environ.get(name))

def load_app_config() -> DictConfig:
    """Loads all configuration files from the core/config directory."""
    config_path = Path("core/config")
    if not config_path.is_dir():
        logger.error(f"Configuration directory not found at '{config_path.resolve()}'")
        raise FileNotFoundError(f"Configuration directory not found at '{config_path.resolve()}'")

    # Load all .yaml files in the config directory
    configs = [OmegaConf.load(p) for p in config_path.glob("*.yaml")]
    # Merge them into a single configuration object
    # Note: If keys overlap, the file loaded last will take precedence.
    return OmegaConf.merge(*configs)

# --- Main Application Logic ---
if __name__ == "__main__":
    logger.info("Application starting up...")
    try:
        # Load the application configuration
        app_config = load_app_config()
        logger.info("Configuration loaded successfully.")
        # Example: Accessing a specific agent's config
        # sql_agent_config = app_config.sql_agent
        # logger.info(f"SQL Agent Config:\n{OmegaConf.to_yaml(sql_agent_config)}")

        # Here you would initialize your main application components/services
        # For example, creating an agent instance:
        # from core.agents import SQLAgent
        # sql_agent = SQLAgent.from_config(
        #     agent_config=app_config.sql_agent,
        #     databases_config=app_config, # Pass the whole config for key-based access
        #     llm_providers_config=app_config,
        #     prompts_base_path=Path("core/prompts")
        # )
        # logger.info("SQLAgent created successfully.")

    except Exception as e:
        logger.critical(f"A critical error occurred during startup: {e}", exc_info=True)
        sys.exit(1) # Exit with a non-zero code to indicate failure


================================================
FILE: agents/__init__.py
================================================
from .sql_agent import SQLAgent

__all__ = ["SQLAgent"]



================================================
FILE: agents/sql_agent.py
================================================
from __future__ import annotations
import logging
from omegaconf import DictConfig
import pandas as pd
from pathlib import Path
from typing import List, TypedDict, Optional, Dict, Any
from langgraph.graph import StateGraph, END

from core.llm import LLMService
from core.database import AgentDatabaseService


class SQLAgentState(TypedDict):
    """
    Represents the state of our SQL Agent.

    Attributes:
        natural_language_question: The initial user question.
        db_context: The database schema and table info.
        history: A log of all attempts and errors.
        max_attempts: The maximum number of retries.
        current_attempt: The current attempt number.
        generated_sql: The SQL query generated by the LLM in the current step.
        final_dataframe: The final, successful result of the query.
        error: The most recent error message, used for conditional routing.
    """
    natural_language_question: str
    db_context: Dict[str, Any]
    history: List[str]
    max_attempts: int
    current_attempt: int
    generated_sql: str
    final_dataframe: Optional[pd.DataFrame]
    error: Optional[str]


logger = logging.getLogger(__name__)


class SQLAgent:
    """
    An agent that uses LangGraph to define a robust, stateful workflow for
    generating, executing, and self-correcting SQL queries.
    """

    def __init__(
        self, 
        llm_service: LLMService, 
        db_service: AgentDatabaseService, 
        name: str,
        max_attempts: int,
    ):
        self.llm_service = llm_service
        self.db_service = db_service
        self.name = name
        self.max_attempts = max_attempts

        # Build and compile the LangGraph workflow
        self.workflow = self._build_graph()
        self.app = self.workflow.compile()

    @classmethod
    def from_config(
        cls, 
        agent_config: DictConfig, 
        databases_config: DictConfig, 
        llm_providers_config: DictConfig, 
        prompts_base_path: Path
    ) -> "SQLAgent":
        """
        Creates an SQLAgent from configuration files.
        """
        # Create LLMService
        llm_service = LLMService(
            agent_prompts_dir=agent_config.prompts_dir,
            provider_key=agent_config.llm_provider_key,
            llm_providers_config=llm_providers_config,
            prompts_base_path=prompts_base_path
        )

        # Create AgentDatabaseService
        db_key = agent_config.database_key
        db_config = databases_config.get(db_key)
        if db_config is None:
            raise ValueError(f"Database key '{db_key}' not found in databases configuration.")
        db_service = AgentDatabaseService(db_config)

        return cls(
            llm_service=llm_service,
            db_service=db_service,
            name=agent_config.name,
            max_attempts=agent_config.max_attempts
        )

    def _build_graph(self) -> StateGraph:
        """Builds the LangGraph StateGraph for the agent's workflow."""
        graph = StateGraph(SQLAgentState)

        # --- 2. Define the Nodes of the Graph ---
        graph.add_node("generate_sql", self.generate_sql_node)
        graph.add_node("execute_sql", self.execute_sql_node)

        # --- 3. Define the Edges of the Graph ---
        graph.set_entry_point("generate_sql")
        graph.add_edge("generate_sql", "execute_sql")
        
        # This conditional edge is the core of the retry loop
        graph.add_conditional_edges(
            "execute_sql",
            self.should_retry_node,
            {
                "retry": "generate_sql",  # If retry is needed, go back to generate
                "end": END               # If successful or out of retries, end
            }
        )
        
        return graph

    # --- Node Function Definitions ---

    def generate_sql_node(self, state: SQLAgentState) -> Dict[str, Any]:
        """
        Node that generates an SQL query using the LLM.
        """
        logger.info(f"--- Attempt {state['current_attempt'] + 1} of {state['max_attempts']}: Generating SQL ---")
        
        llm_variables = {
            "database_dialect": state["db_context"]["db_dialect"],
            "schema_definition": state["db_context"]["table_info"],
            "user_question": state["natural_language_question"],
            "history": "\n".join(state["history"])
        }
        
        generated_sql = self.llm_service.generate_text(llm_variables)
        logger.info(f"Generated SQL:\n{generated_sql}")

        history = state["history"] + [f"ATTEMPT {state['current_attempt'] + 1} SQL:\n{generated_sql}"]
        
        return {
            "current_attempt": state["current_attempt"] + 1,
            "generated_sql": generated_sql,
            "history": history
        }

    def execute_sql_node(self, state: SQLAgentState) -> Dict[str, Any]:
        """
        Node that executes the generated SQL query and handles potential errors.
        """
        generated_sql = state["generated_sql"].strip()

        # First, check if the LLM returned a deliberate error instead of SQL
        if generated_sql.startswith("Error:"):
            logger.warning(f"LLM refused to generate SQL, returning error: {generated_sql}")
            # Set the error to be the exact message from the LLM and stop.
            return {"final_dataframe": None, "error": generated_sql}

        logger.info("--- Executing SQL ---")
        try:
            results_df = self.db_service.execute_for_dataframe(generated_sql)
            logger.info("Successfully executed SQL.")
            return {"final_dataframe": results_df, "error": None}
        except (ValueError, RuntimeError) as e:
            # This is a database execution error
            error_message = f"Error executing SQL: {e}"
            logger.error(f"Execution failed with error:\n{error_message}")
            history = state["history"] + [f"DATABASE ERROR:\n{error_message}"]
            return {"error": error_message, "history": history, "final_dataframe": None}

    def should_retry_node(self, state: SQLAgentState) -> str:
        """
        Conditional edge logic. Determines whether to retry generation or end.
        """
        error = state.get("error")

        if error is None:
            # Success, no error
            logger.info("--- Workflow successful ---")
            return "end"

        # If the error is the exact same as the generated_sql, it means the LLM
        # deliberately returned an error message. We should not retry.
        if error.strip() == state.get("generated_sql").strip():
            logger.error(f"Workflow ended because LLM returned a deliberate error: {error}")
            return "end"

        # If we are out of retries for a database-side error, end.
        if state["current_attempt"] >= state["max_attempts"]:
            logger.warning("-- Max attempts reached, ending workflow --")
            return "end"
        
        # A retry-able error (e.g., from the database) occurred and we have attempts left.
        logger.info(f"--- Database error found, retrying ---")
        return "retry"

    # --- Public Method to Run the Agent ---

    def run(self, natural_language_question: str) -> pd.DataFrame:
        """
        Executes the agent's workflow to answer a natural language question.

        Args:
            natural_language_question: The user's question.

        Returns:
            A pandas DataFrame with the query results.

        Raises:
            RuntimeError: If the agent fails to produce a valid SQL query
                          after all attempts.
        """
        # Prepare the initial state
        initial_state: SQLAgentState = {
            "natural_language_question": natural_language_question,
            "db_context": self.db_service.get_context_for_agent(),
            "history": [],
            "max_attempts": self.max_attempts,
            "current_attempt": 0,
            "generated_sql": "",
            "final_dataframe": None,
            "error": None
        }

        # Invoke the graph
        final_state = self.app.invoke(initial_state)

        # Check the final result
        if final_state["final_dataframe"] is not None:
            return final_state["final_dataframe"]
        else:
            final_error = final_state["error"] or "Unknown error"
            raise RuntimeError(
                f"Agent failed to generate a valid SQL query after {self.max_attempts} attempts. "
                f"Final error: {final_error}"
            )


================================================
FILE: config/agents.yaml
================================================
sql_agent:
  name: "SQLAgent"
  prompts_dir: "sql_agent"
  max_attempts: 3
  llm_provider_key: "azure-openai-gpt4o-mini"
  database_key: "sqlite_default" 


================================================
FILE: config/databases.yaml
================================================
postgres:
  type: postgres
  params:
    host: ${oc.env:DB_HOST}
    port: ${oc.env:DB_PORT}
    user: ${oc.env:DB_USER}
    password: ${oc.env:DB_PASSWORD}
    dbname: ${oc.env:DB_NAME}



================================================
FILE: config/llms.yaml
================================================
llm_providers:
  azure-openai-gpt4o-mini:
    display_name: "Azure OpenAI (GPT-4o mini)"
    class: "langchain_openai.AzureChatOpenAI"
    params:
      temperature: 0.3
      openai_api_version: ${oc.env:OPENAI_API_VERSION}
      azure_deployment: ${oc.env:AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}
      azure_endpoint: ${oc.env:AZURE_OPENAI_ENDPOINT}
      api_key: ${oc.env:AZURE_OPENAI_API_KEY}

  google-gemini-2.5-lite:
    display_name: "Google Gemini (2.5 Flash Lite)"
    class: "langchain_google_genai.ChatGoogleGenerativeAI"
    params:
      temperature: 0.3
      model: "gemini-2.5-flash-lite"
      google_api_key: ${oc.env:GOOGLE_API_KEY}

  google-gemini-2.5-flash:
    display_name: "Google Gemini (2.5 Flash)"
    class: "langchain_google_genai.ChatGoogleGenerativeAI"
    params:
      temperature: 0.3
      model: "gemini-2.5-flash"
      google_api_key: ${oc.env:GOOGLE_API_KEY}
      thinking_budget: 0


================================================
FILE: database/__init__.py
================================================
from .manager import DatabaseManager
from .service import AgentDatabaseService

__all__ = ["DatabaseManager", "AgentDatabaseService"]



================================================
FILE: database/manager.py
================================================
from __future__ import annotations
from omegaconf import DictConfig
from sqlalchemy import create_engine
from sqlalchemy.engine import Engine

from .strategy import DatabaseConnectionStrategy, PostgresConnectionStrategy, SqliteConnectionStrategy

class DatabaseManager:
    """
    Manages database connections via a strategy.
    """
    def __init__(self, db_config: DictConfig):
        """
        Initializes the DatabaseManager with the correct strategy
        based on the provided configuration.
        """
        db_type = db_config.get("type")
        if not db_type:
            raise ValueError("Database 'type' must be specified in the configuration.")

        strategy: DatabaseConnectionStrategy
        params = db_config.get("params", {})

        if db_type == "postgres":
            strategy = PostgresConnectionStrategy(**params)
        elif db_type == "sqlite":
            strategy = SqliteConnectionStrategy(**params)
        else:
            raise NotImplementedError(f"Database type '{db_type}' is not supported.")

        self._strategy = strategy
        self._engine = create_engine(self._strategy.get_uri())

    def get_engine(self) -> Engine:
        """Returns the SQLAlchemy engine instance."""
        return self._engine

    def get_uri(self) -> str:
        """Returns the database connection URI."""
        return self._strategy.get_uri()

    def get_schema_info(self) -> str:
        """Returns the database schema information from the strategy."""
        return self._strategy.get_schema_info()



================================================
FILE: database/service.py
================================================
import pandas as pd
import sqlparse
from omegaconf import DictConfig
from typing import Any, Dict, Optional, List
from sqlalchemy.exc import SQLAlchemyError
from langchain_community.utilities.sql_database import SQLDatabase

from .manager import DatabaseManager

class AgentDatabaseService:
    """
    A self-contained, unified database service that provides interfaces for
    both AI agents (string-based) and internal programmatic use (DataFrame-based).
    """
    def __init__(
        self,
        db_config: DictConfig,
        include_tables: Optional[List[str]] = None,
        sample_rows_in_table_info: int = 3
    ):
        """
        Initializes the service.

        Args:
            db_config: OmegaConf DictConfig containing database connection details.
            include_tables: Optional list of table names to expose to the agent.
            sample_rows_in_table_info: Number of sample rows to include in schema info.
        """
        self.db_type = db_config.get("type"," ")
        self._manager = DatabaseManager(db_config)
        self._engine = self._manager.get_engine()

        self._langchain_db = SQLDatabase(
            engine=self._engine,
            sample_rows_in_table_info=sample_rows_in_table_info,
            include_tables=include_tables,
        )


    def get_context_for_agent(self) -> Dict[str, Any]:
        """Gets all necessary context (schema, tables) for a prompt template."""
        context = {**self._langchain_db.get_context(),"database_dialect": self.db_type}
        return context

    def run_query_for_agent(self, sql_query: str) -> str:
        """
        Executes a query for an agent. Returns results or errors as a string.
        This method is safe and will not crash an agent's tool run.
        """
        return self._langchain_db.run_no_throw(sql_query)


    def execute_for_dataframe(self, sql_query: str) -> pd.DataFrame:
        """
        Executes a read-only SQL query and returns the results as a pandas DataFrame.
        This method contains its own execution logic and is for internal application use.

        Args:
            sql_query: The SQL query string to execute.

        Returns:
            A pandas DataFrame containing the query results.

        Raises:
            ValueError: If the query is not a SELECT statement.
            RuntimeError: If there is a database execution error.
        """
        # A simple security check to prevent modifications
        try:
            statement_type = sqlparse.parse(sql_query)[0].get_type()
            if statement_type != 'SELECT':
                raise ValueError(
                    f"Only SELECT statements are allowed for DataFrame execution. "
                    f"Found statement of type: {statement_type}"
                )
        except IndexError:
            raise ValueError("The SQL query is empty or invalid.")

        # Execute the query and return a DataFrame
        try:
            with self._engine.connect() as connection:
                df = pd.read_sql_query(sql_query, connection)
            return df
        except SQLAlchemyError as e:
            raise RuntimeError(f"Database execution failed: {e}") from e


================================================
FILE: database/strategy.py
================================================
from abc import ABC, abstractmethod
from dataclasses import dataclass

class DatabaseConnectionStrategy(ABC):
    """Abstract base class for a database connection strategy."""
    @abstractmethod
    def get_uri(self) -> str:
        """Constructs the SQLAlchemy database URI."""
        pass

@dataclass
class PostgresConnectionStrategy(DatabaseConnectionStrategy):
    """Strategy for connecting to a PostgreSQL database."""
    host: str
    port: int
    user: str
    password: str
    dbname: str

    def get_uri(self) -> str:
        # Assumes psycopg2 driver is installed (`pip install psycopg2-binary`)
        return f"postgresql+psycopg2://{self.user}:{self.password}@{self.host}:{self.port}/{self.dbname}"

@dataclass
class SqliteConnectionStrategy(DatabaseConnectionStrategy):
    """Strategy for connecting to a SQLite database."""
    db_path: str

    def get_uri(self) -> str:
        return f"sqlite:///{self.db_path}"


================================================
FILE: llm/__init__.py
================================================
from .llm_factory import LLMFactory
from .prompt_manager import PromptManager
from .llm_service import LLMService

__all__ = [
    "LLMFactory",
    "PromptManager",
    "LLMService",
]



================================================
FILE: llm/llm_factory.py
================================================
import importlib
from typing import Dict, Any

from omegaconf import OmegaConf, DictConfig
from langchain_core.language_models.chat_models import BaseChatModel

class LLMFactory:
    """
    A factory class for creating LangChain LLM clients using OmegaConf.
    """

    def __init__(self, llm_providers_config: DictConfig):
        """
        Initializes the factory with the LLM provider configuration.

        Args:
            llm_providers_config: OmegaConf DictConfig containing LLM provider details.
        """
        if not isinstance(llm_providers_config, DictConfig) or 'llm_providers' not in llm_providers_config or not isinstance(llm_providers_config.llm_providers, DictConfig):
            raise ValueError("LLM providers config must be a dictionary and contain a 'llm_providers' dictionary.")
        self._config = llm_providers_config.llm_providers

    def get_available_providers(self) -> Dict[str, str]:
        """
        Returns a dictionary of available provider keys and their display names.
        Reads from the pre-loaded OmegaConf configuration object.
        """
        if not self._config:
            return {}
        return {
            key: provider.get('display_name', key)
            for key, provider in self._config.items()
            if provider
        }

    def create_llm_client(self, provider_key: str) -> BaseChatModel:
        """
        Creates an LLM client instance based on the provider key.

        Args:
            provider_key: The key from the llm_providers.yaml file (e.g., 'azure_openai_4o').

        Returns:
            An instance of the specified LangChain LLM client.
        """
        if not self._config or provider_key not in self._config:
            raise ValueError(f"Provider '{provider_key}' not found in the configuration. "
                             f"Available providers: {list(self._config.keys() if self._config else [])}")

        provider_config = self._config.get(provider_key)

        if 'class' not in provider_config or 'params' not in provider_config:
            raise ValueError(f"Provider '{provider_key}' configuration is missing 'class' or 'params'.")

        resolved_params = OmegaConf.to_container(provider_config.params, resolve=True)

        module_path, class_name = provider_config['class'].rsplit('.', 1)
        try:
            module = importlib.import_module(module_path)
            llm_class = getattr(module, class_name)
        except ImportError:
            raise ImportError(f"Could not import module '{module_path}' for LLM provider '{provider_key}'.")
        except AttributeError:
            raise AttributeError(f"Could not find class '{class_name}' in module '{module_path}'.")
        except Exception as e:
            raise RuntimeError(f"An unexpected error occurred while loading the LLM class: {e}")

        try:
            return llm_class(**resolved_params)
        except TypeError as e:
            raise TypeError(f"Failed to instantiate LLM client for '{provider_key}'. "
                            f"Check if the parameters in the config match the class constructor. Error: {e}")


================================================
FILE: llm/llm_service.py
================================================
from typing import Dict, Any, List, Optional, Type, Union
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from pydantic import BaseModel
from omegaconf import DictConfig
from pathlib import Path

from core.llm.llm_factory import LLMFactory
from core.llm.prompt_manager import PromptManager

class LLMService:
    """
    A high-level interface for AI agents to interact with the LLM.

    This class abstracts away the details of prompt management and LLM client
    creation, providing methods for different types of generation (text, structured).
    """

    def __init__(self, agent_prompts_dir: str, provider_key: str, llm_providers_config: DictConfig, prompts_base_path: Path):
        """
        Initializes the complete LLM stack for a specific agent.

        Args:
            agent_prompts_dir: The name of the agent's prompt directory (e.g., 'sql_generator').
            provider_key: The key for the LLM provider from the config
                          (e.g., 'azure_openai_4o').
            llm_providers_config: OmegaConf DictConfig containing LLM provider details.
            prompts_base_path: Path to the prompts directory.
        """
        # 1. Create LLM client
        llm_factory = LLMFactory(llm_providers_config=llm_providers_config)
        self.llm_client = llm_factory.create_llm_client(provider_key)

        # 2. Load prompts and examples
        prompt_manager = PromptManager(prompts_base_path=prompts_base_path)
        self.system_prompt_template, self.human_prompt_template = prompt_manager.get_prompts(agent_prompts_dir)
        self.few_shot_examples = prompt_manager.get_few_shot_examples(agent_prompts_dir)

    def _build_messages(self, variables: Dict[str, Any]) -> List[BaseMessage]:
        """Helper to build the list of messages for the LLM."""
        messages: List[BaseMessage] = []

        # 1. System Message
        system_content = self.system_prompt_template.format(**variables)
        messages.append(SystemMessage(content=system_content))

        # 2. Few-shot examples
        if self.few_shot_examples:
            for example in self.few_shot_examples:
                if 'user' in example and 'assistant' in example:
                    messages.append(HumanMessage(content=example['user']))
                    messages.append(AIMessage(content=example['assistant']))

        # 3. Human Message
        human_content = self.human_prompt_template.format(**variables)
        messages.append(HumanMessage(content=human_content))
        return messages

    def _extract_sql_from_response(self, raw_response: BaseMessage) -> str:
        """
        Extracts the SQL query from a markdown code block in the raw LLM response.
        Assumes the SQL is within ```sql ... ```.
        """
        start_tag = "```sql"
        end_tag = "```"
        raw_response = raw_response.content

        start_index = raw_response.find(start_tag)
        if start_index == -1:
            # If no start tag, check for error message
            if raw_response.strip().startswith("Error:"):
                return raw_response # Return error message as is
            raise ValueError("LLM response does not contain a '```sql' block.")

        # Adjust start_index to point to the character after the start_tag
        start_index += len(start_tag)

        end_index = raw_response.find(end_tag, start_index)
        if end_index == -1:
            raise ValueError("LLM response contains '```sql' but no closing '```' tag.")

        extracted_sql = raw_response[start_index:end_index].strip()
        return extracted_sql

    def generate_text(self, variables: Dict[str, Any]) -> str:
        """
        Generates a text response from the LLM.
        """
        messages = self._build_messages(variables)
        raw_response = self.llm_client.invoke(messages)
        return self._extract_sql_from_response(raw_response)

    def generate_structured(self, variables: Dict[str, Any], response_model: Type[BaseModel]) -> BaseModel:
        """
        Generates a structured response from the LLM, parsed into a Pydantic model.
        """
        messages = self._build_messages(variables)
        structured_llm = self.llm_client.with_structured_output(response_model)
        structured_response = structured_llm.invoke(messages)
        return structured_response



================================================
FILE: llm/prompt_manager.py
================================================
import json
from pathlib import Path
from typing import Tuple, List, Dict, Optional

class PromptManager:
    """
    Manages loading and providing prompt templates for different agents
    using pathlib for path operations.
    """

    def __init__(self, prompts_base_path: Path):
        """
        Initializes the PromptManager with the base path for prompts.

        Args:
            prompts_base_path: The root directory where agent-specific prompt
                               folders are located.
        """
        if not prompts_base_path.is_dir():
            raise FileNotFoundError(f"Prompts base directory not found at: {prompts_base_path}")
        self.prompts_base_path = prompts_base_path

    def _read_file(self, path: Path) -> str:
        """Reads a file and returns its content."""
        try:
            return path.read_text(encoding='utf-8')
        except FileNotFoundError:
            raise FileNotFoundError(f"Prompt file not found at: {path}")
        except Exception as e:
            raise IOError(f"Error reading prompt file at {path}: {e}")

    def get_prompts(self, prompts_dir: str) -> Tuple[str, str]:
        """
        Loads the system and user prompt templates for a given agent.

        Args:
            prompts_dir: The name of the agent's prompt directory,
                                    corresponding to a folder in the prompts_base_path.

        Returns:
            A tuple containing the system prompt and user prompt templates.
        """
        agent_prompt_dir = self.prompts_base_path / prompts_dir
        if not agent_prompt_dir.is_dir():
            raise FileNotFoundError(f"Prompt directory for agent '{prompts_dir}' not found at {agent_prompt_dir}")

        system_prompt_path = agent_prompt_dir / "system.prompt"
        user_prompt_path = agent_prompt_dir / "user.prompt"

        system_prompt = self._read_file(system_prompt_path)
        user_prompt = self._read_file(user_prompt_path)

        return system_prompt, user_prompt

    def get_few_shot_examples(self, prompts_dir: str) -> Optional[List[Dict[str, str]]]:
        """
        Loads optional few-shot examples for a given agent from a JSON file.

        The JSON file should be a list of dictionaries, where each dictionary
        represents an example (e.g., {"user": "...", "assistant": "..."}).

        Args:
            prompts_dir: The name of the agent's prompt directory.

        Returns:
            A list of few-shot examples, or None if the file doesn't exist.
        """
        examples_path = self.prompts_base_path / prompts_dir / "few_shot_examples.json"
        if not examples_path.exists():
            return None

        try:
            with examples_path.open('r', encoding='utf-8') as f:
                examples = json.load(f)
            if not isinstance(examples, list):
                raise ValueError("Few-shot examples file must contain a JSON list.")
            return examples
        except json.JSONDecodeError as e:
            raise ValueError(f"Error decoding JSON from {examples_path}: {e}")
        except Exception as e:
            raise IOError(f"Error reading few-shot examples file at {examples_path}: {e}")



================================================
FILE: models/__init__.py
================================================
[Empty file]


================================================
FILE: prompts/analysis_reporter/system.prompt
================================================
You are a data analysis expert. Your task is to interpret the results of a SQL query and provide a clear, concise, and insightful analysis for a business user.


================================================
FILE: prompts/analysis_reporter/user.prompt
================================================
Please analyze the following data, which was retrieved using the query:

**SQL Query:**
{sql_query}

**Data:**
{query_results}

**Analysis:**


================================================
FILE: prompts/chart_generator/system.prompt
================================================
You are a data visualization specialist. Your job is to suggest the most appropriate chart to represent a given dataset and provide the necessary configuration in a specified JSON format.


================================================
FILE: prompts/chart_generator/user.prompt
================================================
Given the following data and analysis, please suggest a chart type and provide its configuration.

**Analysis:**
{analysis}

**Data:**
{query_results}

**Chart Configuration (JSON):**


================================================
FILE: prompts/sql_generator/system.prompt
================================================
You are a world-class SQL generation expert. Your sole purpose is to convert natural language questions into accurate, efficient, and executable SQL queries.

You must adhere to the following rules strictly:
1.  **SQL Dialect:** The generated query must be for the {database_dialect} dialect.
2.  **Read-Only Operations:** You must only generate `SELECT` statements. Under no circumstances should you generate any data-modifying statements (`INSERT`, `UPDATE`, `DELETE`, `DROP`, `TRUNCATE`, etc.).
3.  **Schema Adherence:** The query must only use the tables and columns provided in the database schema. Do not invent tables, columns, or functions that are not in the schema.
4.  **Correctness and Efficiency:** The query must be syntactically correct and logically sound. It should answer the user's question accurately. Write efficient queries using CTEs for complex logic and appropriate JOINs.
5.  **Output Format:** You MUST wrap the generated SQL query in a markdown code block like this:
    ```sql
    SELECT ...
    ```
    Your output should contain nothing else outside of this block.
6.  **Handling Ambiguity:** If the user's question is ambiguous or cannot be answered with the provided schema, you must not generate a query. Instead, output a single line starting with "Error:" followed by a concise explanation. For example: "Error: The provided schema does not contain information about customer churn."

Review the provided database schema and the user's question carefully before generating the SQL query.


================================================
FILE: prompts/sql_generator/user.prompt
================================================
**Database Dialect:**
{database_dialect}

**Database Schema:**
```
{schema_definition}
```

**Conversation History (Previous attempts and errors):**
{history}

**User Question:**
{user_question}

**Generated SQL Query:**


================================================
FILE: utils/__init__.py
================================================
from .config_parser import load_config

__all__ = ["load_config"]



================================================
FILE: utils/config_parser.py
================================================
import os
from pathlib import Path
from omegaconf import OmegaConf, DictConfig

def load_config(config_path: Path) -> DictConfig:
    """
    Loads a configuration file using OmegaConf and registers the 'env' resolver.

    Args:
        config_path: The path to the configuration file.

    Returns:
        An OmegaConf DictConfig object.

    Raises:
        FileNotFoundError: If the configuration file does not exist.
        Exception: For other errors during configuration loading.
    """
    if not config_path.is_file():
        raise FileNotFoundError(f"Configuration file not found at: {config_path}")

    # Register the 'env' resolver if not already registered
    # This check prevents re-registration errors if called multiple times
    if not OmegaConf.has_resolver("env"):
        OmegaConf.register_resolver("env", lambda name: os.environ.get(name))

    try:
        config = OmegaConf.load(config_path)
        return config
    except Exception as e:
        raise Exception(f"Error loading configuration from {config_path}: {e}")



================================================
FILE: workflows/__init__.py
================================================
[Empty file]

